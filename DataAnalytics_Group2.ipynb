{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maher\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy import stats\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "get_ipython().magic('matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'NeuronDataMaster2.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f0c1c72c2470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in the data and drop extraneous columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NeuronDataMaster2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maher\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maher\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maher\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maher\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Maher\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'NeuronDataMaster2.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Load in the data and drop extraneous columns\n",
    "df = pd.read_csv('NeuronDataMaster2.csv')\n",
    "df=df.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining a sort function to be used later\n",
    "def Sort(DataType, df):\n",
    "    df=df.sort_values(DataType)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making dataframes to hold the principal cells and the interneurons \n",
    "Principal_Cells = df.where(df.Primary_Cell_Class == 'principal cell').dropna()\n",
    "Interneuron_Cells = df.where(df.Primary_Cell_Class == 'interneuron').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Principal Cells average Number of Branches: ', Principal_Cells.Number_Branches.mean())\n",
    "print('Interneuron Cells average Number of Branches: ', Interneuron_Cells.Number_Branches.mean())\n",
    "print('Principal Cells Number of Branches Standard Deviation: ', Principal_Cells.Number_Branches.std())\n",
    "print('Interneuron Cells Number of Branches Standard Deviation: ', Interneuron_Cells.Number_Branches.std())\n",
    "print(' ')\n",
    "print('Principal Cells average Volume: ', Principal_Cells.Total_Volume.mean())\n",
    "print('Interneuron Cells average Volume: ', Interneuron_Cells.Total_Volume.mean())\n",
    "print('Principal Cells Volume Standard Deviation: ', Principal_Cells.Total_Volume.std())\n",
    "print('Interneuron Cells Volume Standard Deviation: ', Interneuron_Cells.Total_Volume.std())\n",
    "print(' ')\n",
    "print('Principal Cells average Surface: ', Principal_Cells.Total_Surface.mean())\n",
    "print('Interneuron Cells average Surface: ', Interneuron_Cells.Total_Surface.mean())\n",
    "print('Principal Cells Surface Standard Deviation: ', Principal_Cells.Total_Surface.std())\n",
    "print('Interneuron Cells Surface Standard Deviation: ', Interneuron_Cells.Total_Surface.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to compare the mean value of two populations, we must use a hypothesis test. In this test, we use the difference of the \n",
    "#two calculated means and an adjusted standard deviation to see if 0 is within the interval of 90% confidence. If 0 is contained\n",
    "#in the interval, then the test will return false, indicating that no conclusion can be reached (i.e. we cannot reject our null\n",
    "#hypothesis that the are the same).Otherwise, the test indicates there is a significant difference between the two populations\n",
    "def Hypothesis_Test(data1, data2 ):\n",
    "    #All of our Hypothesis tests will use a signifigance level of 0.05 or 90% confidence. \n",
    "    size1 = len(data1.index)\n",
    "    size2 = len(data2.index)\n",
    "    Delta = data1.mean()-data2.mean()\n",
    "    Std = math.pow(math.pow(data1.std(),2)/size1+math.pow(data2.std(),2)/size2,.5)\n",
    "    if (((Delta - 2.132*Std>0)and(Delta +2.132*Std>0))or((Delta - 2.132*Std<0)and(Delta +2.132*Std<0))):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Significant Difference in Number of Branches: ', Hypothesis_Test(Principal_Cells.Number_Branches, Interneuron_Cells.Number_Branches))\n",
    "print('Significant Difference in Volume: ',Hypothesis_Test(Principal_Cells.Total_Volume, Interneuron_Cells.Total_Volume) )\n",
    "print('Significant Difference in Surface: ', Hypothesis_Test(Principal_Cells.Total_Surface, Interneuron_Cells.Total_Surface))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the next 3 cells, we compare all the types of secondary class cells and see if there is a signifigant difference between the\n",
    "#two being compared. Note: the order of the comparison does not affect the result, i.e. A compared to B is the same as B compared\n",
    "# to A. As a result, our loops ony do one comparison be combination\n",
    "Types = df.Secondary_Cell_Class.unique()\n",
    "for n in range(0, len(Types)):\n",
    "    data1 = df.where(df.Secondary_Cell_Class == Types[n]).dropna()\n",
    "    for m in range(n+1, len(Types)):\n",
    "        data2 = df.where(df.Secondary_Cell_Class == Types[m]).dropna()\n",
    "        #print('Significant differences in Number of Branches', Types[n],' ', Types[m],' : ',Hypothesis_Test(data1.Number_Branches,data2.Number_Branches))\n",
    "        if((Hypothesis_Test(data1.Number_Branches,data2.Number_Branches))):\n",
    "            print('Significant differences in Number of Branches', Types[n],' ', Types[m])\n",
    "        else: \n",
    "            print('No Significant differences in Number of Branches', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Types = df.Secondary_Cell_Class.unique()\n",
    "for n in range(0, len(Types)):\n",
    "    data1 = df.where(df.Secondary_Cell_Class == Types[n]).dropna()\n",
    "    for m in range(n+1, len(Types)):\n",
    "        data2 = df.where(df.Secondary_Cell_Class == Types[m]).dropna()\n",
    "        #print('Significant differences in Volume', Types[n],' ', Types[m],' : ',Hypothesis_Test(data1.Total_Volume,data2.Total_Volume))\n",
    "        if(Hypothesis_Test(data1.Total_Volume,data2.Total_Volume)):\n",
    "            print('Significant differences in Volume', Types[n],' ', Types[m])\n",
    "        else:\n",
    "            print('No Significant differences in Volume', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Types = df.Secondary_Cell_Class.unique()\n",
    "for n in range(0, len(Types)):\n",
    "    data1 = df.where(df.Secondary_Cell_Class == Types[n]).dropna()\n",
    "    for m in range(n+1, len(Types)):\n",
    "        data2 = df.where(df.Secondary_Cell_Class == Types[m]).dropna()\n",
    "        #print('Significant differences in Surface', Types[n],' ', Types[m],' : ',Hypothesis_Test(data1.Total_Surface,data2.Total_Surface))\n",
    "        if(Hypothesis_Test(data1.Total_Surface,data2.Total_Surface)):\n",
    "            print('Significant differences in Surface', Types[n],' ', Types[m])\n",
    "        else:\n",
    "            print('No Significant differences in Surface', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since running through ~6000 results for one comparison, we defined a compare function for all three qualities we were hunting\n",
    "def compare_secondary_branches(type1,type2,df):\n",
    "    data1 = df.where(df.Secondary_Cell_Class == type1).dropna()\n",
    "    data2 = df.where(df.Secondary_Cell_Class == type2).dropna()\n",
    "    if(Hypothesis_Test(data1.Number_Branches,data2.Number_Branches)):\n",
    "            print('Significant differences in Branches' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in Branches', type1,' ', type2)\n",
    "def compare_secondary_volume(type1,type2,df):\n",
    "    data1 = df.where(df.Secondary_Cell_Class == type1).dropna()\n",
    "    data2 = df.where(df.Secondary_Cell_Class == type2).dropna()\n",
    "    if(Hypothesis_Test(data1.Total_Volume,data2.Total_Volume)):\n",
    "            print('Significant differences in Volume' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in Volume', type1,' ', type2) \n",
    "def compare_secondary_surface(type1,type2,df):\n",
    "    data1 = df.where(df.Secondary_Cell_Class == type1).dropna()\n",
    "    data2 = df.where(df.Secondary_Cell_Class == type2).dropna()\n",
    "    if(Hypothesis_Test(data1.Total_Surface,data2.Total_Surface)):\n",
    "            print('Significant differences in Surface' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in Surface', type1,' ', type2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we set up to determine if any staining, slicing direction, or thickness provide less data than the others. First we find the\n",
    "#amount of fields nor reported in the data and make a new dataframe that contains the amount of not reports and the stain, slicing \n",
    "#direction, and sliing thickness\n",
    "not_reported_np = np.zeros(shape=(len(df),4))\n",
    "not_reported = pd.DataFrame(not_reported_np)\n",
    "for n in range(0, len(df)):\n",
    "    number_not_reported = 0\n",
    "    for m in range(0,53):\n",
    "        if(df.iat[n,m]=='Not reported'):\n",
    "            number_not_reported += 1\n",
    "    not_reported.set_value(n,0,number_not_reported)\n",
    "not_reported[1]=df.Staining_Method\n",
    "not_reported[2]=df.Slicing_Direction\n",
    "not_reported[3]=df.Slice_Thickness\n",
    "not_reported.columns = ['Number_not_reported', 'Stain','Slice_Direction', 'Slice_Thickness']\n",
    "not_reported = Sort('Stain', not_reported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Types = df.Staining_Method.unique()\n",
    "#We now make a dataframe witht the stain, average amount not reported, and the standard deviation of the not reported. We then take\n",
    "#the minimum in order to find the stain with the least non-reports\n",
    "Averages_np = np.zeros(shape=(len(Types),3))\n",
    "Averages_stain = pd.DataFrame(Averages_np)\n",
    "for n in range(0,len(Types)):\n",
    "    data = not_reported.where(not_reported.Stain == Types[n])\n",
    "    Averages_stain.set_value(n,0,data.mean())\n",
    "    Averages_stain.set_value(n,1,data.std())\n",
    "Averages_stain[2]=Types\n",
    "Averages_stain.columns = ['Average', 'Standard_Deviation', 'Stain']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Minimum_stain = Averages.where(Averages.Average ==Averages.Average.min()).dropna()\n",
    "Minimum_stain\n",
    "#So biotinylated dextran amine gives the least amount of non-reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(0, len(Types)):\n",
    "    data1 = not_reported.where(not_reported.Stain == Types[n]).dropna()\n",
    "    for m in range(n+1, len(Types)):\n",
    "        data2 = not_reported.where(not_reported.Stain == Types[m]).dropna()\n",
    "        if(Hypothesis_Test(data1.Number_not_reported,data2.Number_not_reported)):\n",
    "            print('Significant differences amount reported', Types[n],' ', Types[m])\n",
    "        else:\n",
    "            print('No Significant differences in amounr reported', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Again, since there are many results, we defince search functions in order to quickly find the comparison wanted\n",
    "def compare_stain(type1,type2,df):\n",
    "    data1 = df.where(df.Stain == type1).dropna()\n",
    "    data2 = df.where(df.Stain == type2).dropna()\n",
    "    if(Hypothesis_Test(data1.Number_not_reported,data2.Number_not_reported)):\n",
    "            print('Significant differences in number reported' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in number reported', type1,' ', type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We then apply this to the slicing direction and slicing thickness\n",
    "Types = df.Slicing_Direction.unique()\n",
    "Averages_np = np.zeros(shape=(len(Types),3))\n",
    "Averages_slice_direction = pd.DataFrame(Averages_np)\n",
    "for n in range(0,len(Types)):\n",
    "    data = not_reported.where(not_reported.Slice_Direction == Types[n])\n",
    "    Averages_slice_direction.set_value(n,0,data.mean())\n",
    "    Averages_slice_direction.set_value(n,1,data.std())\n",
    "Averages_slice_direction[2]=Types\n",
    "Averages_slice_direction.columns = ['Average', 'Standard_Deviation', 'Slice_Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Minimum_slicing_direction = Averages_slice_direction.where(Averages_slice_direction.Average ==Averages_slice_direction.Average.min()).dropna()\n",
    "Minimum_slicing_direction\n",
    "#So oblique horizontal cuts gives the least amount of non-reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(0, len(Types)):\n",
    "    data1 = not_reported.where(not_reported.Slice_Direction == Types[n]).dropna()\n",
    "    for m in range(n+1, len(Types)):\n",
    "        data2 = not_reported.where(not_reported.Slice_Direction == Types[m]).dropna()\n",
    "        if(Hypothesis_Test(data1.Number_not_reported,data2.Number_not_reported)):\n",
    "            print('Significant differences amount reported', Types[n],' ', Types[m])\n",
    "        else:\n",
    "            print('No Significant differences in amount reported', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_slicing_direction(type1,type2,df):\n",
    "    data1 = df.where(df.Slice_Direction == type1).dropna()\n",
    "    data2 = df.where(df.Slice_Direction == type2).dropna()\n",
    "    if(Hypothesis_Test(data1.Number_not_reported,data2.Number_not_reported)):\n",
    "            print('Significant differences in number reported' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in number reported', type1,' ', type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We treat the thickness of the cuts as a categorical dataset becuse the values for the thickness is grouped into bins (caused by rounding)\n",
    "#, and as a result, the accuracy of a linear regression is questionable\n",
    "Types = df.Slice_Thickness.unique()\n",
    "Averages_np = np.zeros(shape=(len(Types),3))\n",
    "Averages_slice_thickness = pd.DataFrame(Averages_np)\n",
    "for n in range(0,len(Types)):\n",
    "    data = not_reported.where(not_reported.Slice_Thickness == Types[n])\n",
    "    Averages_slice_thickness.set_value(n,0,data.mean())\n",
    "    Averages_slice_thickness.set_value(n,1,data.std())\n",
    "Averages_slice_thickness[2]=Types\n",
    "Averages_slice_thickness.columns = ['Average', 'Standard_Deviation', 'Slice_Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Minimum_slicing_thickness = Averages_slice_thickness.where(Averages_slice_thickness.Average ==Averages_slice_thickness.Average.min()).dropna()\n",
    "Minimum_slicing_thickness\n",
    "#So 65 thick cuts gives the least amount of non-reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(0, len(Types)):\n",
    "    data1 = not_reported.where(not_reported.Slice_Thickness == Types[n]).dropna()\n",
    "    for m in range(n+1, len(Types)):\n",
    "        data2 = not_reported.where(not_reported.Slice_Thickness == Types[m]).dropna()\n",
    "        if(Hypothesis_Test(data1.Number_not_reported,data2.Number_not_reported)):\n",
    "            print('Significant differences amount reported', Types[n],' ', Types[m])\n",
    "        else:\n",
    "            print('No Significant differences in amount reported', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_slicing_thickness(type1,type2,df):\n",
    "    data1 = df.where(df.Slice_Thickness == type1).dropna()\n",
    "    data2 = df.where(df.Slice_Thickness == type2).dropna()\n",
    "    if(Hypothesis_Test(data1.Number_not_reported,data2.Number_not_reported)):\n",
    "            print('Significant differences in number reported' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in number reported', type1,' ', type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Types = df.Species_Name.unique()\n",
    "for n in range(0,len(Types)):\n",
    "    data1 = df.where(df.Species_Name==Types[n])\n",
    "    data1 = data1.where(data1.Primary_Brain_Region == 'hippocampus')\n",
    "    for m in range(n+1,len(Types)):\n",
    "        data2 = df.where(df.Species_Name==Types[m])\n",
    "        data2 = data2.where(data2.Primary_Brain_Region == 'hippocampus')\n",
    "        if(Hypothesis_Test(data1.Total_Volume,data2.Total_Volume)):\n",
    "            print('Significant differences in Volume', Types[n],' ', Types[m])\n",
    "        else:\n",
    "            print('No Significant differences in Volume', Types[n],' ', Types[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_species(type1,type2,df):\n",
    "    data1 = df.where(df.Species_Name == type1).dropna()\n",
    "    data1 = data1.where(data1.Primary_Brain_Region == 'hippocampus')\n",
    "    data2 = df.where(df.Species_Name == type2).dropna()\n",
    "    data2 = data2.where(data2.Primary_Brain_Region == 'hippocampus')\n",
    "    if(Hypothesis_Test(data1.Total_Volume,data2.Total_Volume)):\n",
    "            print('Significant differences in Volume' , type1,' ', type2)\n",
    "    else:\n",
    "            print('No Significant differences in Volume', type1,' ', type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification function to determine cell class, brain region, and species name by using PCA on morphology data\n",
    "def neuronClassifier(predictors, outputLabel):\n",
    "    predictDf = df[predictors]\n",
    "    \n",
    "    # Standardization to account for different scales of the variables\n",
    "    predictDf_standardized = StandardScaler().fit_transform(predictDf)\n",
    "\n",
    "    outputVector = df[outputLabel]\n",
    "\n",
    "    # PCA to determine the 3 principal components and reduce dimensionality\n",
    "    sklearn_pca = PCA(n_components=3)\n",
    "    Y_sklearn = sklearn_pca.fit_transform(predictDf_standardized)\n",
    "    reducedDf = pd.DataFrame(Y_sklearn)\n",
    "    \n",
    "    # Split the data into testing and training data\n",
    "    testingIndices = []\n",
    "    for x in range(100):\n",
    "        randonum = random.randint(0,len(df.index))\n",
    "        while randonum in testingIndices:\n",
    "            randonum = random.randint(0,len(df.index))\n",
    "        testingIndices.append(randonum)\n",
    "\n",
    "    trainingIndices = [x for x in range(0,len(df.index)) if x not in testingIndices]\n",
    "    testingSet = reducedDf.iloc[testingIndices,:]\n",
    "    trainingSet = reducedDf.iloc[trainingIndices,:]\n",
    "\n",
    "    testLabels = list(outputVector[testingIndices])\n",
    "    trainLabels = list(outputVector[trainingIndices])\n",
    "    \n",
    "    # Generate predictions via majority vote of the 15 nearest neighbors in the training set \n",
    "    predictionVector = []\n",
    "    for i, testInd in enumerate(range(len(testingSet.index))):\n",
    "        print(i)\n",
    "        testRow = testingSet.iloc[testInd,:]\n",
    "        distanceVector = []\n",
    "        for trainInd in range(len(trainingSet.index)):\n",
    "            trainRow = trainingSet.iloc[trainInd,:]\n",
    "            distance = float(euclidean_distances(testRow, trainRow)[0][0])\n",
    "            distanceVector.append((trainInd, distance))\n",
    "        distanceVector = sorted(distanceVector, key = lambda x: x[1])\n",
    "        nearestNeighbors = [x[0] for x in distanceVector[:15]]\n",
    "        nearestNeighborLabels = []\n",
    "        for num in nearestNeighbors:\n",
    "            label = trainLabels[num]\n",
    "            nearestNeighborLabels.append(label)\n",
    "        prediction = stats.mode(nearestNeighborLabels)[0][0]\n",
    "        predictionVector.append(prediction)\n",
    "    \n",
    "    # Compile the results\n",
    "    resultDf = pd.DataFrame(index = testingSet.index, columns = [outputLabel, 'Prediction'])\n",
    "    originalTestingDf = df.iloc[testingSet.index,:]\n",
    "    resultDf['Primary Cell Class'] = originalTestingDf['Primary Cell Class']\n",
    "    resultDf['Prediction'] = predictionVector\n",
    "    \n",
    "    count = 0\n",
    "    for r in list(range(len(resultDf.index))):\n",
    "        if resultDf.iloc[r,0] == resultDf.iloc[r,1]:\n",
    "            count +=1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = count / len(resultDf.index)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test classification algorithm for species name, primary cell class, and primary brain region\n",
    "predictors = ['Soma Surface','Number of Stems', 'Number of Bifurcations', 'Number of Branches', 'Overall Width', 'Overall Height', 'Overall Depth', 'Average Diameter', 'Total Length', 'Total Surface', 'Total Volume', 'Max Euclidean Distance', 'Max Path Distance', 'Max Branch Order', 'Average Contraction', 'Total Fragmentation', 'Partition Asymmetry', 'Average Rall\\'s Ratio', 'Average Bifurcation Angle Local', 'Average Bifurcation Angle Remote', 'Fractal Dimension']\n",
    "\n",
    "speciesAccuracy = neuronClassifier(predictors, 'Species Name')\n",
    "cellClassAccuracy = neuronClassifier(predictors, 'Primary Cell Class')\n",
    "brainRegionAccuracy = neuronClassifier(predictors, 'Primary Brain Region')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification Results\n",
    "print('Species were classified with an accuracy of ' + str(speciesAccuracy))\n",
    "print('Primary Cell Classes were classified with an accuracy of ' + str(cellClassAccuracy))\n",
    "print('Primary Brain Regions were classified with an accuracy of ' + str(brainRegionAccuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
